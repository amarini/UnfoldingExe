\documentclass[a4paper,11pt]{article}

\title{Exercise on unfolding distributions}
\author{ACM}
\date{\today}

%%
\input{difficulty}
\input{packages}
\input{acronyms}


\begin{document}
\maketitle

\section{Introduction}
Unfolding is the ``art'' of being able to undo detector effects, such as smearings, efficiencies or background contamination.
Unfolding techniques refers to a set of statistical tools explained in many books such as Ref.~\cite{Cowan}, 
or summarized in short letter and articles \cite{Cowan:unfolding}. 
Different software suites provide an implementation of such methods; 
in particle physics one of the most spread is {\scshape RooUnfold} \cite{RooUnfold}, 
which can be used within the {\scshape root} software \cite{ROOT}. 

The basic idea behind unfolding is the fact that (at least on certain observables) detector acts linearly on the shapes; this is the case for example for cross-section measurements, where the observables is the total number of event registered in the detector.
The equation we will encounter is the following:
\begin{equation}
	\meas= \respt \cdot \truth + \back
	\label{eqn:base}
\end{equation}
it means that the measured spectra ($\meas$) can be obtained by a smearing and efficiency ($\respt$) applied on the truth distribution ($\truth$). To that a possible background ($\back$) can be added to the measured distribution; this background can come from different sources, like different processes, but also different region of the phase-space where the smearings effects can lead to migration inside the region of interest.

Equation~\ref{eqn:base} can come in different shapes, one of the most relevant is the re-scaling of it with respect to a particular truth/prediction \cite{SVD}:
\begin{equation}
	\meas = \resp \cdot \strength + \back
\end{equation}
where $\strengthI{i} = \frac{\truthI{i}}{\mcI{i}} $ and  $\respI{i}{j} = \resptI{i}{j} \cdot \mcI{i} $, making the content of the response matrix $\resp$ the actual number of expected events.

\subsection{Code and Software}
In HEP the most widespread code is {\scshape root} (and {PyRoot}) with the additional package of {\scshape RooUnfold}.

\section{Understanding unfolding}
\difficulty{2}

The first point of understand how unfolding operate is to deal with the smearing of the distribution.
Suppose to have a fully efficient detector that has just a poor Gaussian resolution of $0.4 a.u.$, as given in the attachments:
\begin{enumerate}
	\item Write the inversion method to unfold the distribution without random fluctuations
	\item Use the inversion method to unfold the distribution with random fluctuations in the attachment; these are obtained with a Gaussian smearing of $1.0$ event.
	\item discuss the result.
\end{enumerate}
\begin{figure}[H]
	\includegraphics[width=0.49\textwidth]{figs/respt.pdf}
	\includegraphics[width=0.49\textwidth]{figs/reco.pdf}
	\caption{
		\label{fig:exe1}
		Left: smearing matrix ($\respt$) 
		Right: reconstructed distributions with and without random fluctuations.
	}	
\end{figure}
\FloatBarrier

\section{Regularization}
\difficulty{3}

The most used regularization methods are the iterative-method based on the Bayes' theorem \cite{dAgostini} and the Tikhonov regularization, which is implemented efficiently with the \gls{SVD} decomposition \cite{SVD}.
Using the histogram with fluctuations of the exercise above:
\begin{enumerate}
	\item provide an unfolded distribution with the different regularization parameters.
	\item discuss the effect and the limits of the regularization parameter in the two methods.
	\item discuss how you would choose the regularization parameter.
\end{enumerate}

\section{Constructing a Response Matrix}
\difficulty{4}

What it usually happens is that response matrices are built from event-based \gls{MC} simulation.
In this section you are asked to unfold a given distribution, constructing before the response matrix.

Event-based \gls{MC} simulations, 
if produced with different luminosity than the one delivered by the machine, needs to be re-weighted 
in order to have the correct \emph{cross-section} and \emph{luminosity} of the data we are comparing to,
Moreover, it is common practice to increase the number of event produced in the \gls{MC} simulation in ``interesting'' 
region of the phase space, therefore these weights are not constant.

Unfortunately, the \gls{MC} simulation is not-perfect and needs to be corrected for small effects.
Typically these are small \emph{scale-factors} measured with techniques like the \emph{tag-and-probe}. 
This type of weights are aimed to correct the difference in efficiency observed in the different objects 
and are assumed to be uncorrelated between the objects.

The scope of this exercise is to simulate the unfolding of a $p_{T}^{Z}$ distribution with the $\mathrm{Z}$ boson decaying into a $\mu$-pair.
In the {\scshape root} file you will find:
\begin{itemize}
	\item The $p_\textup{T}$,$\eta$,$\phi$ of the two muons generated and reconstructed; units are in GeV, and $c=1$
	\item An event weight given by the \gls{MC}-simulation. 
	\item A data distribution to unfold.
\end{itemize}

The following fiducial cuts must be applied on both muons:
\begin{itemize}
	\item $p_\textup{T}^\mu >15 $ GeV 
	\item $|\eta^\mu| < 2.5 $
\end{itemize}

The \emph{scale-factors} ( $\varepsilon_\textup{DATA} / \varepsilon_\textup{MC}$) for the muons efficiencies (scalefactors.txt):
\begin{verbatim}
## pt1 pt2 eta1 eta2 sf
0 10 -5 -2 0.9
0 10 -2 0 0.9
0 10 0 2 0.9
0 10 2 5 0.9
10 20 -5 -2 0.8
10 20 -2 0 0.8
10 20 0 2 0.8
10 20 2 5 0.95
20 50 -5 -2 0.93
20 50 -2 0 0.89
20 50 0 2 0.979
20 50 2 5 0.96
50 10000 -5 -2 0.88
50 10000 -2 0 1.0
50 10000 0 2 1.01
50 10000 2 5 0.96
\end{verbatim}

%%%%%%%%%%%%%%%%%%%% BIBLIO %%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
\nocite{*}
%\bibliographystyle{acm}
\bibliographystyle{acmunsrt}
%\addcontentsline{toc}{section}{\refname}
\bibliography{UnfoldingExe}
%%%%%%%%%%%%%%%%%%
\cleardoublepage

\end{document}
